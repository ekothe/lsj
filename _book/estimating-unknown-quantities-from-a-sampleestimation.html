<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Estimating unknown quantities from a sample{estimation} | Learning statistics with jamovi: a tutorial for psychology students and other beginners (Version 0.70)</title>
  <meta name="description" content="learning statistics with jamovi covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students. The book discusses how to get started in jamovi as well as giving an introduction to data manipulation. From a statistical perspective, the book discusses descriptive statistics and graphing first, followed by chapters on probability theory, sampling and estimation, and null hypothesis testing. After introducing the theory, the book covers the analysis of contingency tables, correlation, \(t\)-tests, regression, ANOVA and factor analysis. Bayesian statistics are covered at the end of the book." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Estimating unknown quantities from a sample{estimation} | Learning statistics with jamovi: a tutorial for psychology students and other beginners (Version 0.70)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="learning statistics with jamovi covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students. The book discusses how to get started in jamovi as well as giving an introduction to data manipulation. From a statistical perspective, the book discusses descriptive statistics and graphing first, followed by chapters on probability theory, sampling and estimation, and null hypothesis testing. After introducing the theory, the book covers the analysis of contingency tables, correlation, \(t\)-tests, regression, ANOVA and factor analysis. Bayesian statistics are covered at the end of the book." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Estimating unknown quantities from a sample{estimation} | Learning statistics with jamovi: a tutorial for psychology students and other beginners (Version 0.70)" />
  
  <meta name="twitter:description" content="learning statistics with jamovi covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students. The book discusses how to get started in jamovi as well as giving an introduction to data manipulation. From a statistical perspective, the book discusses descriptive statistics and graphing first, followed by chapters on probability theory, sampling and estimation, and null hypothesis testing. After introducing the theory, the book covers the analysis of contingency tables, correlation, \(t\)-tests, regression, ANOVA and factor analysis. Bayesian statistics are covered at the end of the book." />
  

<meta name="author" content="Danielle Navarro and David Foxcroft" />


<meta name="date" content="2021-09-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability.html"/>
<link rel="next" href="references.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Overview</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#preface-to-version-0.70"><i class="fa fa-check"></i><b>0.1</b> Preface to Version 0.70</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#preface-to-version-0.65"><i class="fa fa-check"></i><b>0.2</b> Preface to Version 0.65</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#preface-to-version-0.6"><i class="fa fa-check"></i><b>0.3</b> Preface to Version 0.6</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#preface-to-version-0.5"><i class="fa fa-check"></i><b>0.4</b> Preface to Version 0.5</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#preface-to-version-0.4"><i class="fa fa-check"></i><b>0.5</b> Preface to Version 0.4</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#preface-to-version-0.3"><i class="fa fa-check"></i><b>0.6</b> Preface to Version 0.3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-i.-background.html"><a href="part-i.-background.html"><i class="fa fa-check"></i>Part I. Background</a></li>
<li class="chapter" data-level="1" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html"><i class="fa fa-check"></i><b>1</b> Why do we learn statistics?</a>
<ul>
<li class="chapter" data-level="1.1" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#whywhywhy"><i class="fa fa-check"></i><b>1.1</b> On the psychology of statistics</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#the-curse-of-belief-bias"><i class="fa fa-check"></i><b>1.1.1</b> The curse of belief bias</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#the-cautionary-tale-of-simpsons-paradox"><i class="fa fa-check"></i><b>1.2</b> The cautionary tale of Simpson’s paradox</a></li>
<li class="chapter" data-level="1.3" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#statistics-in-psychology"><i class="fa fa-check"></i><b>1.3</b> Statistics in psychology</a></li>
<li class="chapter" data-level="1.4" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#statistics-in-everyday-life"><i class="fa fa-check"></i><b>1.4</b> Statistics in everyday life</a></li>
<li class="chapter" data-level="1.5" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#theres-more-to-research-methods-than-statistics"><i class="fa fa-check"></i><b>1.5</b> There’s more to research methods than statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="studydesign.html"><a href="studydesign.html"><i class="fa fa-check"></i><b>2</b> A brief introduction to research design</a>
<ul>
<li class="chapter" data-level="2.1" data-path="studydesign.html"><a href="studydesign.html#measurement"><i class="fa fa-check"></i><b>2.1</b> Introduction to psychological measurement</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="studydesign.html"><a href="studydesign.html#some-thoughts-about-psychological-measurement"><i class="fa fa-check"></i><b>2.1.1</b> Some thoughts about psychological measurement</a></li>
<li class="chapter" data-level="2.1.2" data-path="studydesign.html"><a href="studydesign.html#operationalisation-defining-your-measurement"><i class="fa fa-check"></i><b>2.1.2</b> Operationalisation: defining your measurement</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="studydesign.html"><a href="studydesign.html#scales"><i class="fa fa-check"></i><b>2.2</b> Scales of measurement</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="studydesign.html"><a href="studydesign.html#nominal-scale"><i class="fa fa-check"></i><b>2.2.1</b> Nominal scale</a></li>
<li class="chapter" data-level="2.2.2" data-path="studydesign.html"><a href="studydesign.html#ordinal-scale"><i class="fa fa-check"></i><b>2.2.2</b> Ordinal scale</a></li>
<li class="chapter" data-level="2.2.3" data-path="studydesign.html"><a href="studydesign.html#interval-scale"><i class="fa fa-check"></i><b>2.2.3</b> Interval scale</a></li>
<li class="chapter" data-level="2.2.4" data-path="studydesign.html"><a href="studydesign.html#ratio-scale"><i class="fa fa-check"></i><b>2.2.4</b> Ratio scale</a></li>
<li class="chapter" data-level="2.2.5" data-path="studydesign.html"><a href="studydesign.html#continuousdiscrete"><i class="fa fa-check"></i><b>2.2.5</b> Continuous versus discrete variables</a></li>
<li class="chapter" data-level="2.2.6" data-path="studydesign.html"><a href="studydesign.html#some-complexities"><i class="fa fa-check"></i><b>2.2.6</b> Some complexities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="studydesign.html"><a href="studydesign.html#reliability"><i class="fa fa-check"></i><b>2.3</b> Assessing the reliability of a measurement</a></li>
<li class="chapter" data-level="2.4" data-path="studydesign.html"><a href="studydesign.html#ivdv"><i class="fa fa-check"></i><b>2.4</b> The “role” of variables: predictors and outcomes</a></li>
<li class="chapter" data-level="2.5" data-path="studydesign.html"><a href="studydesign.html#researchdesigns"><i class="fa fa-check"></i><b>2.5</b> Experimental and non-experimental research</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="studydesign.html"><a href="studydesign.html#experimental-research"><i class="fa fa-check"></i><b>2.5.1</b> Experimental research</a></li>
<li class="chapter" data-level="2.5.2" data-path="studydesign.html"><a href="studydesign.html#non-experimental-research"><i class="fa fa-check"></i><b>2.5.2</b> Non-experimental research</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="studydesign.html"><a href="studydesign.html#validity"><i class="fa fa-check"></i><b>2.6</b> Assessing the validity of a study</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="studydesign.html"><a href="studydesign.html#internal-validity"><i class="fa fa-check"></i><b>2.6.1</b> Internal validity</a></li>
<li class="chapter" data-level="2.6.2" data-path="studydesign.html"><a href="studydesign.html#external-validity"><i class="fa fa-check"></i><b>2.6.2</b> External validity</a></li>
<li class="chapter" data-level="2.6.3" data-path="studydesign.html"><a href="studydesign.html#construct-validity"><i class="fa fa-check"></i><b>2.6.3</b> Construct validity</a></li>
<li class="chapter" data-level="2.6.4" data-path="studydesign.html"><a href="studydesign.html#face-validity"><i class="fa fa-check"></i><b>2.6.4</b> Face validity</a></li>
<li class="chapter" data-level="2.6.5" data-path="studydesign.html"><a href="studydesign.html#ecological-validity"><i class="fa fa-check"></i><b>2.6.5</b> Ecological validity</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="studydesign.html"><a href="studydesign.html#confounds-artifacts-and-other-threats-to-validity"><i class="fa fa-check"></i><b>2.7</b> Confounds, artifacts and other threats to validity</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="studydesign.html"><a href="studydesign.html#history-effects"><i class="fa fa-check"></i><b>2.7.1</b> History effects</a></li>
<li class="chapter" data-level="2.7.2" data-path="studydesign.html"><a href="studydesign.html#maturation-effects"><i class="fa fa-check"></i><b>2.7.2</b> Maturation effects</a></li>
<li class="chapter" data-level="2.7.3" data-path="studydesign.html"><a href="studydesign.html#repeated-testing-effects"><i class="fa fa-check"></i><b>2.7.3</b> Repeated testing effects</a></li>
<li class="chapter" data-level="2.7.4" data-path="studydesign.html"><a href="studydesign.html#selection-bias"><i class="fa fa-check"></i><b>2.7.4</b> Selection bias</a></li>
<li class="chapter" data-level="2.7.5" data-path="studydesign.html"><a href="studydesign.html#differentialattrition"><i class="fa fa-check"></i><b>2.7.5</b> Differential attrition</a></li>
<li class="chapter" data-level="2.7.6" data-path="studydesign.html"><a href="studydesign.html#non-response-bias"><i class="fa fa-check"></i><b>2.7.6</b> Non-response bias</a></li>
<li class="chapter" data-level="2.7.7" data-path="studydesign.html"><a href="studydesign.html#regression-to-the-mean"><i class="fa fa-check"></i><b>2.7.7</b> Regression to the mean</a></li>
<li class="chapter" data-level="2.7.8" data-path="studydesign.html"><a href="studydesign.html#experimenter-bias"><i class="fa fa-check"></i><b>2.7.8</b> Experimenter bias</a></li>
<li class="chapter" data-level="2.7.9" data-path="studydesign.html"><a href="studydesign.html#demand-effects-and-reactivity"><i class="fa fa-check"></i><b>2.7.9</b> Demand effects and reactivity</a></li>
<li class="chapter" data-level="2.7.10" data-path="studydesign.html"><a href="studydesign.html#placebo-effects"><i class="fa fa-check"></i><b>2.7.10</b> Placebo effects</a></li>
<li class="chapter" data-level="2.7.11" data-path="studydesign.html"><a href="studydesign.html#situation-measurement-and-subpopulation-effects"><i class="fa fa-check"></i><b>2.7.11</b> Situation, measurement and subpopulation effects</a></li>
<li class="chapter" data-level="2.7.12" data-path="studydesign.html"><a href="studydesign.html#fraud-deception-and-self-deception"><i class="fa fa-check"></i><b>2.7.12</b> Fraud, deception and self-deception</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="studydesign.html"><a href="studydesign.html#summary"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii.-an-introduction-to-jamovi.html"><a href="part-ii.-an-introduction-to-jamovi.html"><i class="fa fa-check"></i>Part II. An introduction to jamovi</a></li>
<li class="chapter" data-level="3" data-path="introj.html"><a href="introj.html"><i class="fa fa-check"></i><b>3</b> Getting started with jamovi</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introj.html"><a href="introj.html#gettingjamovi"><i class="fa fa-check"></i><b>3.1</b> Installing jamovi</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introj.html"><a href="introj.html#starting-up-jamovi"><i class="fa fa-check"></i><b>3.1.1</b> Starting up jamovi</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="introj.html"><a href="introj.html#analyses"><i class="fa fa-check"></i><b>3.2</b> Analyses</a></li>
<li class="chapter" data-level="3.3" data-path="introj.html"><a href="introj.html#spreadsheet"><i class="fa fa-check"></i><b>3.3</b> The spreadsheet</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="introj.html"><a href="introj.html#variables"><i class="fa fa-check"></i><b>3.3.1</b> Variables</a></li>
<li class="chapter" data-level="3.3.2" data-path="introj.html"><a href="introj.html#computed-variables"><i class="fa fa-check"></i><b>3.3.2</b> Computed variables</a></li>
<li class="chapter" data-level="3.3.3" data-path="introj.html"><a href="introj.html#copypaste"><i class="fa fa-check"></i><b>3.3.3</b> Copy and Paste</a></li>
<li class="chapter" data-level="3.3.4" data-path="introj.html"><a href="introj.html#syntaxmode"><i class="fa fa-check"></i><b>3.3.4</b> Syntax mode</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introj.html"><a href="introj.html#load"><i class="fa fa-check"></i><b>3.4</b> Loading data in jamovi</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="introj.html"><a href="introj.html#importing-data-from-csv-files"><i class="fa fa-check"></i><b>3.4.1</b> Importing data from csv files</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="introj.html"><a href="introj.html#importing"><i class="fa fa-check"></i><b>3.5</b> Importing unusual data files</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="introj.html"><a href="introj.html#loading-data-from-text-files"><i class="fa fa-check"></i><b>3.5.1</b> Loading data from text files</a></li>
<li class="chapter" data-level="3.5.2" data-path="introj.html"><a href="introj.html#loading-data-from-spss-and-other-statistics-packages"><i class="fa fa-check"></i><b>3.5.2</b> Loading data from SPSS (and other statistics packages)</a></li>
<li class="chapter" data-level="3.5.3" data-path="introj.html"><a href="introj.html#loading-excel-files"><i class="fa fa-check"></i><b>3.5.3</b> Loading Excel files</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="introj.html"><a href="introj.html#coercion"><i class="fa fa-check"></i><b>3.6</b> Changing data from one level to another</a></li>
<li class="chapter" data-level="3.7" data-path="introj.html"><a href="introj.html#jamovimodules"><i class="fa fa-check"></i><b>3.7</b> Installing add-on modules into jamovi</a></li>
<li class="chapter" data-level="3.8" data-path="introj.html"><a href="introj.html#quittingjamovi"><i class="fa fa-check"></i><b>3.8</b> Quitting jamovi</a></li>
<li class="chapter" data-level="3.9" data-path="introj.html"><a href="introj.html#summary-1"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iii.-working-with-data.html"><a href="part-iii.-working-with-data.html"><i class="fa fa-check"></i>Part III. Working with data</a></li>
<li class="chapter" data-level="4" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>4</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="descriptives.html"><a href="descriptives.html#centraltendency"><i class="fa fa-check"></i><b>4.1</b> Measures of central tendency</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="descriptives.html"><a href="descriptives.html#mean"><i class="fa fa-check"></i><b>4.1.1</b> The mean</a></li>
<li class="chapter" data-level="4.1.2" data-path="descriptives.html"><a href="descriptives.html#calculating-the-mean-in-jamovi"><i class="fa fa-check"></i><b>4.1.2</b> Calculating the mean in jamovi</a></li>
<li class="chapter" data-level="4.1.3" data-path="descriptives.html"><a href="descriptives.html#median"><i class="fa fa-check"></i><b>4.1.3</b> The median</a></li>
<li class="chapter" data-level="4.1.4" data-path="descriptives.html"><a href="descriptives.html#mean-or-median-whats-the-difference"><i class="fa fa-check"></i><b>4.1.4</b> Mean or median? What’s the difference?</a></li>
<li class="chapter" data-level="4.1.5" data-path="descriptives.html"><a href="descriptives.html#housingpriceexample"><i class="fa fa-check"></i><b>4.1.5</b> A real life example</a></li>
<li class="chapter" data-level="4.1.6" data-path="descriptives.html"><a href="descriptives.html#mode"><i class="fa fa-check"></i><b>4.1.6</b> Mode</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="descriptives.html"><a href="descriptives.html#var"><i class="fa fa-check"></i><b>4.2</b> Measures of variability</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="descriptives.html"><a href="descriptives.html#range"><i class="fa fa-check"></i><b>4.2.1</b> Range</a></li>
<li class="chapter" data-level="4.2.2" data-path="descriptives.html"><a href="descriptives.html#interquartile-range"><i class="fa fa-check"></i><b>4.2.2</b> Interquartile range</a></li>
<li class="chapter" data-level="4.2.3" data-path="descriptives.html"><a href="descriptives.html#aad"><i class="fa fa-check"></i><b>4.2.3</b> Mean absolute deviation</a></li>
<li class="chapter" data-level="4.2.4" data-path="descriptives.html"><a href="descriptives.html#variance"><i class="fa fa-check"></i><b>4.2.4</b> Variance</a></li>
<li class="chapter" data-level="4.2.5" data-path="descriptives.html"><a href="descriptives.html#sd"><i class="fa fa-check"></i><b>4.2.5</b> Standard deviation</a></li>
<li class="chapter" data-level="4.2.6" data-path="descriptives.html"><a href="descriptives.html#which-measure-to-use"><i class="fa fa-check"></i><b>4.2.6</b> Which measure to use?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="descriptives.html"><a href="descriptives.html#skewkurt"><i class="fa fa-check"></i><b>4.3</b> Skew and kurtosis</a></li>
<li class="chapter" data-level="4.4" data-path="descriptives.html"><a href="descriptives.html#groupdescriptives"><i class="fa fa-check"></i><b>4.4</b> Descriptive statistics separately for each group</a></li>
<li class="chapter" data-level="4.5" data-path="descriptives.html"><a href="descriptives.html#zscore"><i class="fa fa-check"></i><b>4.5</b> Standard scores</a></li>
<li class="chapter" data-level="4.6" data-path="descriptives.html"><a href="descriptives.html#summarydesc"><i class="fa fa-check"></i><b>4.6</b> Summary</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="descriptives.html"><a href="descriptives.html#epilogue-good-descriptive-statistics-are-descriptive"><i class="fa fa-check"></i><b>4.6.1</b> Epilogue: Good descriptive statistics are descriptive!</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="graphics.html"><a href="graphics.html"><i class="fa fa-check"></i><b>5</b> Drawing graphs</a>
<ul>
<li class="chapter" data-level="5.1" data-path="graphics.html"><a href="graphics.html#hist"><i class="fa fa-check"></i><b>5.1</b> Histograms</a></li>
<li class="chapter" data-level="5.2" data-path="graphics.html"><a href="graphics.html#boxplots"><i class="fa fa-check"></i><b>5.2</b> Boxplots</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="graphics.html"><a href="graphics.html#violinplots"><i class="fa fa-check"></i><b>5.2.1</b> Violin plots</a></li>
<li class="chapter" data-level="5.2.2" data-path="graphics.html"><a href="graphics.html#multipleboxplots"><i class="fa fa-check"></i><b>5.2.2</b> Drawing multiple boxplots</a></li>
<li class="chapter" data-level="5.2.3" data-path="graphics.html"><a href="graphics.html#boxplotoutliers"><i class="fa fa-check"></i><b>5.2.3</b> Using box plots to detect outliers</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="graphics.html"><a href="graphics.html#bargraph"><i class="fa fa-check"></i><b>5.3</b> Bar graphs</a></li>
<li class="chapter" data-level="5.4" data-path="graphics.html"><a href="graphics.html#saveimage"><i class="fa fa-check"></i><b>5.4</b> Saving image files using jamovi</a></li>
<li class="chapter" data-level="5.5" data-path="graphics.html"><a href="graphics.html#summary-2"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="datahandling.html"><a href="datahandling.html"><i class="fa fa-check"></i><b>6</b> Pragmatic matters</a>
<ul>
<li class="chapter" data-level="6.1" data-path="datahandling.html"><a href="datahandling.html#freqtables"><i class="fa fa-check"></i><b>6.1</b> Tabulating and cross-tabulating data</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="datahandling.html"><a href="datahandling.html#creating-tables-for-single-variables"><i class="fa fa-check"></i><b>6.1.1</b> Creating tables for single variables</a></li>
<li class="chapter" data-level="6.1.2" data-path="datahandling.html"><a href="datahandling.html#adding-percentages-to-a-contingency-table"><i class="fa fa-check"></i><b>6.1.2</b> Adding percentages to a contingency table</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="datahandling.html"><a href="datahandling.html#logicals"><i class="fa fa-check"></i><b>6.2</b> Logical expressions in jamovi</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="datahandling.html"><a href="datahandling.html#assessing-mathematical-truths"><i class="fa fa-check"></i><b>6.2.1</b> Assessing mathematical truths</a></li>
<li class="chapter" data-level="6.2.2" data-path="datahandling.html"><a href="datahandling.html#logical-operations"><i class="fa fa-check"></i><b>6.2.2</b> Logical operations</a></li>
<li class="chapter" data-level="6.2.3" data-path="datahandling.html"><a href="datahandling.html#logictext"><i class="fa fa-check"></i><b>6.2.3</b> Applying logical operation to text</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="datahandling.html"><a href="datahandling.html#transform"><i class="fa fa-check"></i><b>6.3</b> Transforming and recoding a variable</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="datahandling.html"><a href="datahandling.html#creating-a-transformed-variable"><i class="fa fa-check"></i><b>6.3.1</b> Creating a transformed variable</a></li>
<li class="chapter" data-level="6.3.2" data-path="datahandling.html"><a href="datahandling.html#collapsing-a-variable-into-a-smaller-number-of-discrete-levels-or-categories"><i class="fa fa-check"></i><b>6.3.2</b> Collapsing a variable into a smaller number of discrete levels or categories</a></li>
<li class="chapter" data-level="6.3.3" data-path="datahandling.html"><a href="datahandling.html#creating-a-transformation-that-can-be-applied-to-multiple-variables"><i class="fa fa-check"></i><b>6.3.3</b> Creating a transformation that can be applied to multiple variables</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="datahandling.html"><a href="datahandling.html#mathfunc"><i class="fa fa-check"></i><b>6.4</b> A few more mathematical functions and operations</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="datahandling.html"><a href="datahandling.html#logarithms-and-exponentials"><i class="fa fa-check"></i><b>6.4.1</b> Logarithms and exponentials</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="datahandling.html"><a href="datahandling.html#subset"><i class="fa fa-check"></i><b>6.5</b> Extracting a subset of the data</a></li>
<li class="chapter" data-level="6.6" data-path="datahandling.html"><a href="datahandling.html#summary-3"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>7</b> Introduction to probability</a>
<ul>
<li class="chapter" data-level="7.1" data-path="probability.html"><a href="probability.html#probstats"><i class="fa fa-check"></i><b>7.1</b> How are probability and statistics different?</a></li>
<li class="chapter" data-level="7.2" data-path="probability.html"><a href="probability.html#probmeaning"><i class="fa fa-check"></i><b>7.2</b> What does probability mean?</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="probability.html"><a href="probability.html#the-frequentist-view"><i class="fa fa-check"></i><b>7.2.1</b> The frequentist view</a></li>
<li class="chapter" data-level="7.2.2" data-path="probability.html"><a href="probability.html#the-bayesian-view"><i class="fa fa-check"></i><b>7.2.2</b> The Bayesian view</a></li>
<li class="chapter" data-level="7.2.3" data-path="probability.html"><a href="probability.html#whats-the-difference-and-who-is-right"><i class="fa fa-check"></i><b>7.2.3</b> What’s the difference? And who is right?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="probability.html"><a href="probability.html#basicprobability"><i class="fa fa-check"></i><b>7.3</b> Basic probability theory</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="probability.html"><a href="probability.html#introducing-probability-distributions"><i class="fa fa-check"></i><b>7.3.1</b> Introducing probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="probability.html"><a href="probability.html#binomial"><i class="fa fa-check"></i><b>7.4</b> The binomial distribution</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="probability.html"><a href="probability.html#introducing-the-binomial"><i class="fa fa-check"></i><b>7.4.1</b> Introducing the binomial</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="probability.html"><a href="probability.html#normal"><i class="fa fa-check"></i><b>7.5</b> The normal distribution</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="probability.html"><a href="probability.html#density"><i class="fa fa-check"></i><b>7.5.1</b> Probability density</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="probability.html"><a href="probability.html#otherdists"><i class="fa fa-check"></i><b>7.6</b> Other useful distributions</a></li>
<li class="chapter" data-level="7.7" data-path="probability.html"><a href="probability.html#summary-4"><i class="fa fa-check"></i><b>7.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="estimating-unknown-quantities-from-a-sampleestimation.html"><a href="estimating-unknown-quantities-from-a-sampleestimation.html"><i class="fa fa-check"></i><b>8</b> Estimating unknown quantities from a sample{estimation}</a>
<ul>
<li class="chapter" data-level="8.1" data-path="estimating-unknown-quantities-from-a-sampleestimation.html"><a href="estimating-unknown-quantities-from-a-sampleestimation.html#samples-populations-and-samplingsrs"><i class="fa fa-check"></i><b>8.1</b> Samples, populations and sampling{srs}</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="estimating-unknown-quantities-from-a-sampleestimation.html"><a href="estimating-unknown-quantities-from-a-sampleestimation.html#defining-a-populationpop"><i class="fa fa-check"></i><b>8.1.1</b> Defining a population{pop}</a></li>
<li class="chapter" data-level="8.1.2" data-path="estimating-unknown-quantities-from-a-sampleestimation.html"><a href="estimating-unknown-quantities-from-a-sampleestimation.html#simple-random-samples"><i class="fa fa-check"></i><b>8.1.2</b> Simple random samples</a></li>
<li class="chapter" data-level="8.1.3" data-path="estimating-unknown-quantities-from-a-sampleestimation.html"><a href="estimating-unknown-quantities-from-a-sampleestimation.html#most-samples-are-not-simple-random-samples"><i class="fa fa-check"></i><b>8.1.3</b> Most samples are not simple random samples</a></li>
<li class="chapter" data-level="8.1.4" data-path="estimating-unknown-quantities-from-a-sampleestimation.html"><a href="estimating-unknown-quantities-from-a-sampleestimation.html#how-much-does-it-matter-if-you-dont-have-a-simple-random-sample"><i class="fa fa-check"></i><b>8.1.4</b> How much does it matter if you don’t have a simple random sample?</a></li>
<li class="chapter" data-level="8.1.5" data-path="estimating-unknown-quantities-from-a-sampleestimation.html"><a href="estimating-unknown-quantities-from-a-sampleestimation.html#population-parameters-and-sample-statistics"><i class="fa fa-check"></i><b>8.1.5</b> Population parameters and sample statistics</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="estimating-unknown-quantities-from-a-sampleestimation.html"><a href="estimating-unknown-quantities-from-a-sampleestimation.html#the-law-of-large-numberslawlargenumbers"><i class="fa fa-check"></i><b>8.2</b> The law of large numbers{lawlargenumbers}</a></li>
<li class="chapter" data-level="8.3" data-path="estimating-unknown-quantities-from-a-sampleestimation.html"><a href="estimating-unknown-quantities-from-a-sampleestimation.html#sampling-distributions-and-the-central-limit-theoremsamplesandclt"><i class="fa fa-check"></i><b>8.3</b> Sampling distributions and the central limit theorem{samplesandclt}</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="estimating-unknown-quantities-from-a-sampleestimation.html"><a href="estimating-unknown-quantities-from-a-sampleestimation.html#sampling-distribution-of-the-meansamplingdists"><i class="fa fa-check"></i><b>8.3.1</b> Sampling distribution of the mean{samplingdists}</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="estimating-unknown-quantities-from-a-sampleestimation.html"><a href="estimating-unknown-quantities-from-a-sampleestimation.html#summary-5"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Learning statistics with jamovi: a tutorial for psychology students and other beginners (Version 0.70)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimating-unknown-quantities-from-a-sampleestimation" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> Estimating unknown quantities from a sample{estimation}</h1>
<p>At the start of the last chapter I highlighted the critical distinction between <em>descriptive statistics</em> and <em>inferential statistics</em>. As discussed in Chapter <a href="descriptives.html#descriptives">4</a>, the role of descriptive statistics is to concisely summarise what we <em>do</em> know. In contrast, the purpose of inferential statistics is to ‘learn what we do not know from what we do.’ Now that we have a foundation in probability theory we are in a good position to think about the problem of statistical inference. What kinds of things would we like to learn about? And how do we learn them? These are the questions that lie at the heart of inferential statistics, and they are traditionally divided into two ‘big ideas’: estimation and hypothesis testing. The goal in this chapter is to introduce the first of these big ideas, estimation theory, but I’m going to witter on about sampling theory first because estimation theory doesn’t make sense until you understand sampling. As a consequence, this chapter divides naturally into two parts Sections <a href="#srs"><strong>??</strong></a> through <a href="#samplesandclt"><strong>??</strong></a> are focused on sampling theory, and Sections <a href="#pointestimates"><strong>??</strong></a> and <a href="#ci"><strong>??</strong></a> make use of sampling theory to discuss how statisticians think about estimation.</p>
<div id="samples-populations-and-samplingsrs" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Samples, populations and sampling{srs}</h2>
<p>In the prelude to Part I discussed the riddle of induction and highlighted the fact that <em>all</em> learning requires you to make assumptions. Accepting that this is true, our first task to come up with some fairly general assumptions about data that make sense. This is where <strong><em>sampling theory</em></strong> comes in. If probability theory is the foundations upon which all statistical theory builds, sampling theory is the frame around which you can build the rest of the house. Sampling theory plays a huge role in specifying the assumptions upon which your statistical inferences rely. And in order to talk about ‘making inferences’ the way statisticians think about it we need to be a bit more explicit about what it is that we’re drawing inferences <em>from</em> (the sample) and what it is that we’re drawing inferences <em>about</em> (the population).</p>
<p>In almost every situation of interest what we have available to us as researchers is a <strong><em>sample</em></strong> of data. We might have run experiment with some number of participants, a polling company might have phoned some number of people to ask questions about voting intentions, and so on. In this way the data set available to us is finite and incomplete. We can’t possibly get every person in the world to do our experiment, for example a polling company doesn’t have the time or the money to ring up every voter in the country. In our earlier discussion of descriptive statistics (Chapter <a href="descriptives.html#descriptives">4</a>) this sample was the only thing we were interested in. Our only goal was to find ways of describing, summarising and graphing that sample. This is about to change.</p>
<div id="defining-a-populationpop" class="section level3" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Defining a population{pop}</h3>
<p>A sample is a concrete thing. You can open up a data file and there’s the data from your sample. A <strong><em>population</em></strong>, on the other hand, is a more abstract idea. It refers to the set of all possible people, or all possible observations, that you want to draw conclusions about and is generally <em>much</em> bigger than the sample. In an ideal world the researcher would begin the study with a clear idea of what the population of interest is, since the process of designing a study and testing hypotheses with the data does depend on the population about which you want to make statements.</p>
<p>Sometimes it’s easy to state the population of interest. For instance, in the ‘polling company’ example that opened the chapter the population consisted of all voters enrolled at the time of the study, millions of people. The sample was a set of 1000 people who all belong to that population. In most studies the situation is much less straightforward. In a typical psychological experiment determining the population of interest is a bit more complicated. Suppose I run an experiment using 100 undergraduate students as my participants. My goal, as a cognitive scientist, is to try to learn something about how the mind works. So, which of the following would count as ‘the population’:</p>
<ul>
<li>All of the undergraduate psychology students at the University of Adelaide?</li>
<li>Undergraduate psychology students in general, anywhere in the world?</li>
<li>Australians currently living?</li>
<li>Australians of similar ages to my sample?</li>
<li>Anyone currently alive?</li>
<li>Any human being, past, present or future?</li>
<li>Any biological organism with a sufficient degree of intelligence operating in a terrestrial environment?</li>
<li>Any intelligent being?</li>
</ul>
<p>Each of these defines a real group of mind-possessing entities, all of which might be of interest to me as a cognitive scientist, and it’s not at all clear which one ought to be the true population of interest. As another example, consider the Wellesley-Croker game that we discussed in the prelude. The sample here is a specific sequence of 12 wins and 0 losses for Wellesley. What is the population?</p>
<ul>
<li>All outcomes until Wellesley and Croker arrived at their destination?</li>
<li>All outcomes if Wellesley and Croker had played the game for the rest of their lives?</li>
<li>All outcomes if Wellseley and Croker lived forever and played the game until the world ran out of hills?</li>
<li>All outcomes if we created an infinite set of parallel universes and the Wellesely/Croker pair made guesses about the same 12 hills in each universe?</li>
</ul>
<p>Again, it’s not obvious what the population is.</p>
</div>
<div id="simple-random-samples" class="section level3" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> Simple random samples</h3>
<p>Irrespective of how I define the population, the critical point is that the sample is a subset of the population and our goal is to use our knowledge of the sample to draw inferences about the properties of the population. The relationship between the two depends on the <em>procedure</em> by which the sample was selected. This procedure is referred to as a <strong><em>sampling method</em></strong> and it is important to understand why it matters.</p>
<p>To keep things simple, let’s imagine that we have a bag containing 10 chips. Each chip has a unique letter printed on it so we can distinguish between the 10 chips. The chips come in two colours, black and white. This set of chips is the population of interest and it is depicted graphically on the left of Figure <a href="#fig:srs1"><strong>??</strong></a>. As you can see from looking at the picture there are 4 black chips and 6 white chips, but of course in real life we wouldn’t know that unless we looked in the bag. Now imagine you run the following ‘experiment’: you shake up the bag, close your eyes, and pull out 4 chips without putting any of them back into the bag. First out comes the <span class="math inline">\(a\)</span> chip (black), then the <span class="math inline">\(c\)</span> chip (white), then <span class="math inline">\(j\)</span> (white) and then finally <span class="math inline">\(b\)</span> (black). If you wanted you could then put all the chips back in the bag and repeat the experiment, as depicted on the right hand side of Figure <a href="#fig:srs1"><strong>??</strong></a>. Each time you get different results but the procedure is identical in each case. The fact that the same procedure can lead to different results each time we refer to as a <em>random</em> process.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> However, because we shook the bag before pulling any chips out, it seems reasonable to think that every chip has the same chance of being selected. A procedure in which every member of the population has the same chance of being selected is called a <strong><em>simple random sample</em></strong>. The fact that we did <em>not</em> put the chips back in the bag after pulling them out means that you can’t observe the same thing twice, and in such cases the observations are said to have been sampled <strong><em>without replacement</em></strong>.</p>
<p>To help make sure you understand the importance of the sampling procedure, consider an alternative way in which the experiment could have been run. Suppose that my 5-year old son had opened the bag and decided to pull out four black chips without putting any of them back in the bag. This <em>biased</em> sampling scheme is depicted in Figure <a href="#fig:brs"><strong>??</strong></a>. Now consider the evidential value of seeing 4 black chips and 0 white chips. Clearly it depends on the sampling scheme, does it not? If you know that the sampling scheme is biased to select only black chips then a sample that consists of only black chips doesn’t tell you very much about the population! For this reason statisticians really like it when a data set can be considered a simple random sample, because it makes the data analysis <em>much</em> easier.</p>
\begin{figure}[t]
<p>replacement from a finite population}


\end{figure}</p>
<p>A third procedure is worth mentioning. This time around we close our eyes, shake the bag, and pull out a chip. This time, however, we record the observation and then put the chip back in the bag. Again we close our eyes, shake the bag, and pull out a chip. We then repeat this procedure until we have 4 chips. Data sets generated in this way are still simple random samples, but because we put the chips back in the bag immediately after drawing them it is referred to as a sample <strong><em>with replacement</em></strong>. The difference between this situation and the first one is that it is possible to observe the same population member multiple times, as illustrated in Figure <a href="#fig:srs2"><strong>??</strong></a>.</p>
<p>In my experience, most psychology experiments tend to be sampling without replacement, because the same person is not allowed to participate in the experiment twice. However, most statistical theory is based on the assumption that the data arise from a simple random sample <em>with</em> replacement. In real life this very rarely matters. If the population of interest is large (e.g., has more than 10 entities!) the difference between sampling with- and without- replacement is too small to be concerned with. The difference between simple random samples and biased samples, on the other hand, is not such an easy thing to dismiss.</p>
</div>
<div id="most-samples-are-not-simple-random-samples" class="section level3" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> Most samples are not simple random samples</h3>
<p>As you can see from looking at the list of possible populations that I showed above, it is almost impossible to obtain a simple random sample from most populations of interest. When I run experiments I’d consider it a minor miracle if my participants turned out to be a random sampling of the undergraduate psychology students at Adelaide university, even though this is by far the narrowest population that I might want to generalise to. A thorough discussion of other types of sampling schemes is beyond the scope of this book, but to give you a sense of what’s out there I’ll list a few of the more important ones.</p>
<ul>
<li><em>Stratified sampling</em>. Suppose your population is (or can be) divided into several different sub-populations, or <em>strata</em>. Perhaps you’re running a study at several different sites, for example. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. Stratified sampling is sometimes easier to do than simple random sampling, especially when the population is already divided into the distinct strata. It can also be more efficient than simple random sampling, especially when some of the sub-populations are rare. For instance, when studying schizophrenia it would be much better to divide the population into two<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> strata (schizophrenic and not-schizophrenic) and then sample an equal number of people from each group. If you selected people randomly you would get so few schizophrenic people in the sample that your study would be useless. This specific kind of of stratified sampling is referred to as <em>oversampling</em> because it makes a deliberate attempt to over-represent rare groups.</li>
<li><em>Snowball sampling</em> is a technique that is especially useful when sampling from a ‘hidden’ or hard to access population and is especially common in social sciences. For instance, suppose the researchers want to conduct an opinion poll among transgender people. The research team might only have contact details for a few trans folks, so the survey starts by asking them to participate (stage 1). At the end of the survey the participants are asked to provide contact details for other people who might want to participate. In stage 2 those new contacts are surveyed. The process continues until the researchers have sufficient data. The big advantage to snowball sampling is that it gets you data in situations that might otherwise be impossible to get any. On the statistical side, the main disadvantage is that the sample is highly non-random, and non-random in ways that are difficult to address. On the real life side, the disadvantage is that the procedure can be unethical if not handled well, because hidden populations are often hidden for a reason. I chose transgender people as an example here to highlight this issue. If you weren’t careful you might end up outing people who don’t want to be outed (very, very bad form), and even if you don’t make that mistake it can still be intrusive to use people’s social networks to study them. It’s certainly very hard to get people’s informed consent <em>before</em> contacting them, yet in many cases the simple act of contacting them and saying ‘hey we want to study you’ can be hurtful. Social networks are complex things, and just because you can use them to get data doesn’t always mean you should.</li>
<li><em>Convenience sampling</em> is more or less what it sounds like. The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Snowball sampling is one type of convenience sampling, but there are many others. A common example in psychology are studies that rely on undergraduate psychology students. These samples are generally non-random in two respects. First, reliance on undergraduate psychology students automatically means that your data are restricted to a single sub-population. Second, the students usually get to pick which studies they participate in, so the sample is a self selected subset of psychology students and not a randomly selected subset. In real life most studies are convenience samples of one form or another. This is sometimes a severe limitation, but not always.</li>
</ul>
</div>
<div id="how-much-does-it-matter-if-you-dont-have-a-simple-random-sample" class="section level3" number="8.1.4">
<h3><span class="header-section-number">8.1.4</span> How much does it matter if you don’t have a simple random sample?</h3>
<p>Okay, so real world data collection tends not to involve nice simple random samples. Does that matter? A little thought should make it clear to you that it <em>can</em> matter if your data are not a simple random sample. Just think about the difference between Figures <a href="#fig:srs1"><strong>??</strong></a> and <a href="#fig:brs"><strong>??</strong></a>. However, it’s not quite as bad as it sounds. Some types of biased samples are entirely unproblematic. For instance, when using a stratified sampling technique you actually <em>know</em> what the bias is because you created it deliberately, often to <em>increase</em> the effectiveness of your study, and there are statistical techniques that you can use to adjust for the biases you’ve introduced (not covered in this book!). So in those situations it’s not a problem.</p>
<p>More generally though, it’s important to remember that random sampling is a means to an end, and not the end in itself. Let’s assume you’ve relied on a convenience sample, and as such you can assume it’s biased. A bias in your sampling method is only a problem if it causes you to draw the wrong conclusions. When viewed from that perspective, I’d argue that we don’t need the sample to be randomly generated in <em>every</em> respect, we only need it to be random with respect to the psychologically-relevant phenomenon of interest. Suppose I’m doing a study looking at working memory capacity. In study 1, I actually have the ability to sample randomly from all human beings currently alive, with one exception: I can only sample people born on a Monday. In study 2, I am able to sample randomly from the Australian population. I want to generalise my results to the population of all living humans. Which study is better? The answer, obviously, is study 1. Why? Because we have no reason to think that being ‘born on a Monday’ has any interesting relationship to working memory capacity. In contrast, I can think of several reasons why ‘being Australian’ might matter. Australia is a wealthy, industrialised country with a very well-developed education system. People growing up in that system will have had life experiences much more similar to the experiences of the people who designed the tests for working memory capacity. This shared experience might easily translate into similar beliefs about how to ‘take a test,’ a shared assumption about how psychological experimentation works, and so on. These things might actually matter. For instance, ‘test taking’ style might have taught the Australian participants how to direct their attention exclusively on fairly abstract test materials much more than people who haven’t grown up in a similar environment. This could therefore lead to a misleading picture of what working memory capacity is.</p>
<p>There are two points hidden in this discussion. First, when designing your own studies, it’s important to think about what population you care about and try hard to sample in a way that is appropriate to that population. In practice, you’re usually forced to put up with a ‘sample of convenience’ (e.g., psychology lecturers sample psychology students because that’s the least expensive way to collect data, and our coffers aren’t exactly overflowing with gold), but if so you should at least spend some time thinking about what the dangers of this practice might be. Second, if you’re going to criticise someone else’s study because they’ve used a sample of convenience rather than laboriously sampling randomly from the entire human population, at least have the courtesy to offer a specific theory as to <em>how</em> this might have distorted the results.</p>
</div>
<div id="population-parameters-and-sample-statistics" class="section level3" number="8.1.5">
<h3><span class="header-section-number">8.1.5</span> Population parameters and sample statistics</h3>
<p>Okay. Setting aside the thorny methodological issues associated with obtaining a random sample, let’s consider a slightly different issue. Up to this point we have been talking about populations the way a scientist might. To a psychologist a population might be a group of people. To an ecologist a population might be a group of bears. In most cases the populations that scientists care about are concrete things that actually exist in the real world. Statisticians, however, are a funny lot. On the one hand, they <em>are</em> interested in real world data and real science in the same way that scientists are. On the other hand, they also operate in the realm of pure abstraction in the way that mathematicians do. As a consequence, statistical theory tends to be a bit abstract in how a population is defined. In much the same way that psychological researchers operationalise our abstract theoretical ideas in terms of concrete measurements (Section <a href="studydesign.html#measurement">2.1</a>), statisticians operationalise the concept of a ‘population’ in terms of mathematical objects that they know how to work with. You’ve already come across these objects in Chapter <a href="probability.html#probability">7</a>. They’re called probability distributions.</p>
<p>The idea is quite simple. Let’s say we’re talking about IQ scores. To a psychologist the population of interest is a group of actual humans who have IQ scores. A statistician ‘simplifies’ this by operationally defining the population as the probability distribution depicted in Figure <a href="#fig:IQdist"><strong>??</strong></a>a. IQ tests are designed so that the average IQ is 100, the standard deviation of IQ scores is 15, and the distribution of IQ scores is normal. These values are referred to as the <strong><em>population parameters</em></strong> because they are characteristics of the entire population. That is, we say that the population mean <span class="math inline">\(\mu\)</span> is 100 and the population standard deviation <span class="math inline">\(\sigma\)</span> is 15.</p>
<p>Now suppose I run an experiment. I select 100 people at random and administer an IQ test, giving me a simple random sample from the population. My sample would consist of a collection of numbers like this:</p>
<pre><code>                      `106 101 98 80 74 ... 107 72 100`</code></pre>
<p>Each of these IQ scores is sampled from a normal distribution with mean 100 and standard deviation 15. So if I plot a histogram of the sample I get something like the one shown in Figure <a href="#fig:IQdist"><strong>??</strong></a>b. As you can see, the histogram is <em>roughly</em> the right shape but it’s a very crude approximation to the true population distribution shown in Figure <a href="#fig:IQdist"><strong>??</strong></a>a. When I calculate the mean of my sample, I get a number that is fairly close to the population mean 100 but not identical. In this case, it turns out that the people in my sample have a mean IQ of 98.5, and the standard deviation of their IQ scores is 15.9. These <strong><em>sample statistics</em></strong> are properties of my data set, and although they are fairly similar to the true population values they are not the same. In general, sample statistics are the things you can calculate from your data set and the population parameters are the things you want to learn about. Later on in this chapter I’ll talk about how you can estimate population parameters using your sample statistics (Section <a href="#pointestimates"><strong>??</strong></a>) and how to work out how confident you are in your estimates (Section <a href="#ci"><strong>??</strong></a>) but before we get to that there’s a few more ideas in sampling theory that you need to know about.</p>
</div>
</div>
<div id="the-law-of-large-numberslawlargenumbers" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> The law of large numbers{lawlargenumbers}</h2>
<p>In the previous section I showed you the results of one fictitious IQ experiment with a sample size of <span class="math inline">\(N=100\)</span>. The results were somewhat encouraging as the true population mean is 100 and the sample mean of 98.5 is a pretty reasonable approximation to it. In many scientific studies that level of precision is perfectly acceptable, but in other situations you need to be a lot more precise. If we want our sample statistics to be much closer to the population parameters, what can we do about it?</p>
<p>The obvious answer is to collect more data. Suppose that we ran a much larger experiment, this time measuring the IQs of 10,000 people. We can simulate the results of this experiment using jamovi. The <code>IQsim.omv</code> file is a jamovi data file. In this file I have generated 10,000 random numbers sampled from a normal distribution for a population with <code>mean = 100</code> and <code>sd = 15</code>. This was done by computing a new variable using the <code>= NORM(100,15)</code> function. A histogram and density plot shows that this larger sample is a much better approximation to the true population distribution than the smaller one. This is reflected in the sample statistics. The mean IQ for the larger sample turns out to be 99.68 and the standard deviation is 14.90. These values are now very close to the true population. See Figure <a href="#fig:iqsim"><strong>??</strong></a></p>
<p>I feel a bit silly saying this, but the thing I want you to take away from this is that large samples generally give you better information. I feel silly saying it because it’s so bloody obvious that it shouldn’t need to be said. In fact, it’s such an obvious point that when Jacob Bernoulli, one of the founders of probability theory, formalised this idea back in 1713 he was kind of a jerk about it. Here’s how he described the fact that we all share this intuition:</p>
<blockquote>
<p><em>For even the most stupid of men, by some instinct of nature, by himself and without any instruction (which is a remarkable thing), is convinced that the more observations have been made, the less danger there is of wandering from one’s goal</em>
<span class="citation">(see <a href="#ref-Stigler1986" role="doc-biblioref">Stigler 1986, p65</a>)</span></p>
</blockquote>
<p>Okay, so the passage comes across as a bit condescending (not to mention sexist), but his main point is correct. It really does feel obvious that more data will give you better answers. The question is, why is this so? Not surprisingly, this intuition that we all share turns out to be correct, and statisticians refer to it as the <strong><em>law of large numbers</em></strong>. The law of large numbers is a mathematical law that applies to many different sample statistics but the simplest way to think about it is as a law about averages. The sample mean is the most obvious example of a statistic that relies on averaging (because that’s what the mean is… an average), so let’s look at that. When applied to the sample mean what the law of large numbers states is that as the sample gets larger, the sample mean tends to get closer to the true population mean. Or, to say it a little bit more precisely, as the sample size ‘approaches’ infinity (written as <span class="math inline">\(N \rightarrow \infty\)</span>), the sample mean approaches the population mean (<span class="math inline">\(\bar{X} \rightarrow \mu\)</span>).^[Technically, the law of large numbers pertains to any sample statistic that can be described as an average of independent quantities. That’s certainly true for the sample mean. However, it’s also possible to write many other sample statistics as averages of one form or another. The variance of a sample, for instance, can be rewritten as a kind of average and so is subject to the law of large numbers. The minimum value of a sample, however, cannot be written as an average of anything and is therefore not governed by the law of large numbers.]</p>
<p>I don’t intend to subject you to a proof that the law of large numbers is true, but it’s one of the most important tools for statistical theory. The law of large numbers is the thing we can use to justify our belief that collecting more and more data will eventually lead us to the truth. For any particular data set the sample statistics that we calculate from it will be wrong, but the law of large numbers tells us that if we keep collecting more data those sample statistics will tend to get closer and closer to the true population parameters.</p>
</div>
<div id="sampling-distributions-and-the-central-limit-theoremsamplesandclt" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Sampling distributions and the central limit theorem{samplesandclt}</h2>
<p>The law of large numbers is a very powerful tool but it’s not going to be good enough to answer all our questions. Among other things, all it gives us is a ‘long run guarantee.’ In the long run, if we were somehow able to collect an infinite amount of data, then the law of large numbers guarantees that our sample statistics will be correct. But as John Maynard Keynes famously argued in economics, a long run guarantee is of little use in real life.</p>
<blockquote>
<p><em>[The] long run is a misleading guide to current affairs. In the long run we are all dead. Economists set themselves too easy, too useless a task, if in tempestuous seasons they can only tell us, that when the storm is long past, the ocean is flat again.</em>
–<span class="citation">(<a href="#ref-Keynes1923" role="doc-biblioref">Keynes 1923, ~80</a>)</span></p>
</blockquote>
<p>As in economics, so too in psychology and statistics. It is not enough to know that we will <em>eventually</em> arrive at the right answer when calculating the sample mean. Knowing that an infinitely large data set will tell me the exact value of the population mean is cold comfort when my <em>actual</em> data set has a sample size of <span class="math inline">\(N=100\)</span>. In real life, then, we must know something about the behaviour of the sample mean when it is calculated from a more modest data set!</p>
<div id="sampling-distribution-of-the-meansamplingdists" class="section level3" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> Sampling distribution of the mean{samplingdists}</h3>
<p>With this in mind, let’s abandon the idea that our studies will have sample sizes of 10,000 and consider instead a very modest experiment indeed. This time around we’ll sample <span class="math inline">\(N=5\)</span> people and measure their IQ scores. As before, I can simulate this experiment in jamovi <code>= NORM(100,15)</code> function, but I only need 5 participant IDs this time, not 10,000. These are the five numbers that jamovi generated:</p>
<p><code>90 82 94 99 110</code></p>
<p>The mean IQ in this sample turns out to be exactly 95. Not surprisingly, this is much less accurate than the previous experiment. Now imagine that I decided to <strong><em>replicate</em></strong> the experiment. That is, I repeat the procedure as closely as possible and I randomly sample 5 new people and measure their IQ. Again, jamovi allows me to simulate the results of this procedure, and generates these five numbers:</p>
<p><code>78 88 111 111 117</code></p>
<p>This time around, the mean IQ in my sample is 101. If I repeat the experiment 10 times I obtain the results shown in Table~</p>
<p>As far as I can tell, jamovi does not (yet) include a simple way to calculate confidence intervals for the mean as part of the ‘Descriptives’ functionality. But the ‘Descriptives’ do have a check box for the S.E. Mean, so you can use this to calculate the lower 95% confidence interval as:</p>
<p><code>Mean - (1.96 * S.E. Mean)</code> , and the upper 95% confidence interval as:</p>
<p><code>Mean + (1.96 * S.E. Mean)</code></p>
<p>95% confidence intervals are the de facto standard in psychology. So, for example, if I load the <code>IQsim.omv</code> file, check mean and S.E mean under ’Descriptives, I can work out the confidence interval associated with the simulated mean IQ:</p>
<p>Lower 95% CI = 99.68 - (1.96 * 0.15) = 99.39</p>
<p>Upper 95% CI = 99.68 + (1.96 * 0.15) = 99.98</p>
<p>So, in our simulated large sample data with N=10,000, the mean IQ score is 99.68 with a 95% CI from 99.39 to 99.98. Hopefully that’s fairly clear. So, although there currently is not a straightforward way to get jamovi to calculate the confidence interval as part of the variable ‘Descriptives’ options, if we wanted to we could pretty easily work it out by hand.</p>
<p>Similarly, when it comes to plotting confidence intervals in jamovi, this is not (yet) available as part of the ‘Descriptives’ options. However, when we get onto learning about specific statistical tests, for example in Chapter \ref{ch:anova), we will see that we can plot confidence intervals as part of the data analysis. That’s pretty cool, so we’ll show you how to do that later on.</p>
</div>
</div>
<div id="summary-5" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Summary</h2>
<p>In this chapter I’ve covered two main topics. The first half of the chapter talks about sampling theory, and the second half talks about how we can use sampling theory to construct estimates of the population parameters. The section breakdown looks like this:</p>
<ul>
<li>Basic ideas about samples, sampling and populations (Section <a href="#srs"><strong>??</strong></a>)</li>
<li>Statistical theory of sampling: the law of large numbers (Section <a href="#lawlargenumbers"><strong>??</strong></a>), sampling distributions and the central limit theorem (Section <a href="#samplesandclt"><strong>??</strong></a>).</li>
<li>Estimating means and standard deviations (Section <a href="#pointestimates"><strong>??</strong></a>)</li>
<li>Estimating a confidence interval (Section <a href="#ci"><strong>??</strong></a>)</li>
</ul>
<p>As always, there’s a lot of topics related to sampling and estimation that aren’t covered in this chapter, but for an introductory psychology class this is fairly comprehensive I think. For most applied researchers you won’t need much more theory than this. One big question that I haven’t touched on in this chapter is what you do when you don’t have a simple random sample. There is a lot of statistical theory you can draw on to handle this situation, but it’s well beyond the scope of this book.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Keynes1923" class="csl-entry">
Keynes, John Maynard. 1923. <em>A Tract on Monetary Reform</em>. London: Macmillan; Company.
</div>
<div id="ref-Stigler1986" class="csl-entry">
Stigler, S. M. 1986. <em>The History of Statistics</em>. Cambridge, MA: Harvard University Press.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>The proper mathematical definition of randomness is extraordinarily technical, and way beyond the scope of this book. We’ll be non-technical here and say that a process has an element of randomness to it whenever it is possible to repeat the process and get different answers each time.<a href="estimating-unknown-quantities-from-a-sampleestimation.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Nothing in life is that simple. There’s not an obvious division of people into binary categories like ‘schizophrenic’ and ‘not schizophrenic.’ But this isn’t a clinical psychology text so please forgive me a few simplifications here and there.<a href="estimating-unknown-quantities-from-a-sampleestimation.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["lsj.pdf", "lsj.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
